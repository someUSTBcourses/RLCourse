MC算法的一个最最最最大的缺陷是，它没有充分利用到历史数据。
它是基于新数据的迭代，新数据得到了之后，旧数据就全都不要了，
这导致极大概率采样采到不好的新数据，也更新进去了，就特别特别愚蠢。

后面所学的SARSA，就弥补了这个缺陷，它是通过【旧数据-（学习率 * 新数据）】计算的，并且能证明其必定收敛于目标期望

回看老师第五次课，回看到ε-greedy的方法，其实感觉有很大的问题
这一张讲的是蒙特卡洛的方法，把model base的 值迭代/策略迭代算法，优化为基于数据的 model free的方法
那么就需要用采样，去得到近似的action-value的值

而MC-basic是直接对于每个state-action-pair都采一个很长的trajectory

MC-exploring-starts的核心是exploring+starts,即对于每个state-action-pair都要有访问到，最简单的方法就是像MC-basic一样，直接弄个二重循环（这就和MC-basic没区别了）。还有一种方法是对于一个state-action-pair进行足够长的trajectory的采样，可以访问到很多节点。 之后再去单独访问未访问过的节点即可，这样会节省一些训练时间。

再之后有一个ε-greedy的方法。在学完后面回看这节课，发现ε的存在，就只是为了探索而已。就是为了遍历到所有的state-action-pair。因为只要探索到一次，只要trajectory足够长，那么它采样得到的action-value足够精确的。

但是ε-greedy，他的探索和收敛，是矛盾的。非常矛盾，因为这样探索得到结果之后，还需要用ε的概率，去生成一个stochastic的策略，但这个策略不比之前的优秀多少（再怎么优秀，都是基于ε的，每次迭代ε+=0.01，策略最多只能优秀0.几），这就很矛盾，明明已经知道采样得到当前的action-value值了，直接更新一个deteministic的策略，直接就能收敛了。但是如果是ε-greedy 就疯狂曲线绕路，还可能不收敛，收敛全靠天

所以后面的Q-learning才是正解，把探索和更新兼顾到了，就很帅
