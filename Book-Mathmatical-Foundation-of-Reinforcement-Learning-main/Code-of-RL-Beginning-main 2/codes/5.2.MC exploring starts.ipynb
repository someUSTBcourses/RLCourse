{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a129cdd-2f42-41ca-a367-20e73e3e160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     #只需要下载numpy库即可\n",
    "import random\n",
    "import GridWorld_v2\n",
    "# import set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10109ffe-ac93-4120-b750-62a6263f7594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "⬆️⬆️⬅️⬇️🔄\n",
      "➡️⏫️⏪⬆️➡️\n",
      "⬇️➡️⏫️➡️⬇️\n",
      "⬇️🔄✅⏬⬆️\n",
      "🔄⏫️🔄⬅️⬇️\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9   #折扣因子，越接近0越近视\n",
    "\n",
    "rows = 5      #记得行数和列数这里要同步改\n",
    "columns = 5\n",
    "\n",
    "# gridworld = GridWorld_v2.GridWorld_v2(rows=rows, columns=columns, forbiddenAreaNums=8, targetNums=2, seed = 52,forbiddenAreaScore=-10)\n",
    "# gridworld = GridWorld_v2.GridWorld_v2(desc = [\".#\",\".T\"])             #赵老师4-1的例子\n",
    "# gridworld = GridWorld_v2.GridWorld_v2(desc = [\"##.T\",\"...#\",\"....\"])  #随便弄的例子\n",
    "gridworld = GridWorld_v2.GridWorld_v2(forbiddenAreaScore=-10, score=1,desc = [\".....\",\".##..\",\"..#..\",\".#T#.\",\".#...\"]) \n",
    "#gridworld = GridWorld_v2.GridWorld_v2(forbiddenAreaScore=-10, score=1,desc = [\"T.\"]) \n",
    "gridworld.show()\n",
    "\n",
    "value = np.zeros(rows*columns)       #初始化可以任意，也可以全0\n",
    "qtable = np.zeros((rows*columns,5))  #初始化，这里主要是初始化维数，里面的内容会被覆盖所以无所谓\n",
    "\n",
    "# np.random.seed(50)\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*columns))] \n",
    "gridworld.showPolicy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04199d4d-c797-4075-801e-a469ec189ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "⬆️⬅️⬇️⬅️⬅️\n",
      "🔄⏫️🔄🔄⬆️\n",
      "🔄⬇️⏩️⬅️🔄\n",
      "⬅️⏩️✅🔄⬅️\n",
      "⬅️⏪➡️🔄⬇️\n",
      "random policy\n",
      "⬇️🔄⬅️⬇️⬇️\n",
      "⬇️⏪⏩️🔄⬇️\n",
      "⬆️⬅️⏬⬆️🔄\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean -31.66122803357247\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 0.29277346688155703\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 0.5552036922867665\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 0.791389917691976\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 2.8998281594850077\n",
      "➡️➡️➡️⬇️⬇️\n",
      "⬆️⏫️⏩️⬇️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 2.9688942318450082\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "action value's mean 2.968894231845008\n"
     ]
    }
   ],
   "source": [
    "# Every-visited  ，下一个cell是first-visited\n",
    "#通过采样的方法计算action value，model free的话意味着不知道整个gridworld的概率了，所以不能直接套贝尔曼方程迭代求解\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*columns))] \n",
    "gridworld.show()\n",
    "gridworld.showPolicy(policy)\n",
    "print(\"random policy\")\n",
    "\n",
    "\n",
    "trajectorySteps = 100\n",
    "qtable = np.zeros((rows*columns,5))    #生成Qtable，也就是action-value-table\n",
    "qtable_pre = qtable.copy()+1\n",
    "while(np.sum((qtable_pre-qtable)**2)>0.001):\n",
    "    # print(np.sum((qtable_pre-qtable)**2))\n",
    "    qtable_pre = qtable.copy()\n",
    "    #通过采样获得action-value的值\n",
    "    for i in range(rows * columns):    #循环每个state\n",
    "        for j in range(5):             #循环每个action\n",
    "            qtable_rewards = [[0 for j in range(5)] for i in range(rows * columns)] \n",
    "            qtable_nums =    [[0 for j in range(5)] for i in range(rows * columns)]\n",
    "            Trajectory = gridworld.getTrajectoryScore(nowState=i, action=j, policy=policy, steps=trajectorySteps)\n",
    "            # 注意这里的返回值是大小为(trajectorySteps+1)的元组列表，因为把第一个动作也加入进去了\n",
    "            _, _, score, _, _ = Trajectory[trajectorySteps]\n",
    "            for k in range(trajectorySteps-1,-1,-1):\n",
    "                tmpstate, tmpaction, tmpscore,_,_  = Trajectory[k]\n",
    "                score = score*gamma + tmpscore  #细节从后往前优化算法\n",
    "                qtable_rewards[tmpstate][tmpaction] += score\n",
    "                qtable_nums[tmpstate][tmpaction] += 1\n",
    "                qtable[tmpstate][tmpaction] = qtable_rewards[tmpstate][tmpaction] / qtable_nums[tmpstate][tmpaction]\n",
    "                #every visit\n",
    "            # qtable[i,j] = score\n",
    "\n",
    "    # print(\"qtable[0]:\", qtable[0])\n",
    "    # print(\"qtable[1]:\", qtable[1])\n",
    "    policy = np.eye(5)[np.argmax(qtable,axis=1)]  #qtable的最优值作为更新策略，并用独热码来表示\n",
    "    \n",
    "    gridworld.showPolicy( policy)\n",
    "    print(\"action value's mean\",qtable.mean())\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce921c3-3b3c-464b-9b0c-b0e52aa87523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "⬆️⬇️⬆️🔄🔄\n",
      "🔄⏩️⏫️⬇️⬇️\n",
      "➡️⬇️⏬➡️⬇️\n",
      "⬆️⏪✅⏩️➡️\n",
      "🔄⏩️⬅️⬅️🔄\n",
      "random policy\n",
      "125.0\n",
      "qtable[0]: [ -9.99973439 -23.66076095   0.          -9.99976095  -8.99976095]\n",
      "qtable[1]: [-24.66076095  -8.99976095 -26.28973439  -8.99976095 -23.66076095]\n",
      "⬇️➡️➡️➡️⬅️\n",
      "🔄⏪⏬⬆️⬆️\n",
      "⬆️➡️⏬⬅️⬆️\n",
      "⬇️⏩️✅⏪⬇️\n",
      "🔄⏪⬆️➡️🔄\n",
      "58724.65101582276\n",
      "qtable[0]: [-1.  0.  0. -1.  0.]\n",
      "qtable[1]: [ -1.   0. -10.   0.   0.]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬆️\n",
      "54289.483682466176\n",
      "qtable[0]: [-1.  0.  0. -1.  0.]\n",
      "qtable[1]: [ -1.   0. -10.   0.   0.]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "668.2177293353504\n",
      "qtable[0]: [-1.  0.  0. -1.  0.]\n",
      "qtable[1]: [ -1.   0. -10.   0.   0.]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "265.70307335590013\n",
      "qtable[0]: [-1.  0.  0. -1.  0.]\n",
      "qtable[1]: [ -1.   0. -10.   0.   0.]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "215.21792104600723\n",
      "qtable[0]: [-1.  0.  0. -1.  0.]\n",
      "qtable[1]: [ -1.   0. -10.   0.   0.]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "174.3251045200501\n",
      "qtable[0]: [2.13786691 3.48637956 2.82405631 2.13786691 3.13786691]\n",
      "qtable[1]: [ 2.48654535  3.87375507 -6.86213309  3.13786691  3.48654535]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "1005.1913240876647\n",
      "qtable[0]: [2.13786691 3.48637956 2.82405631 2.13786691 3.13786691]\n",
      "qtable[1]: [ 2.48654535  3.87375507 -6.86213309  3.13786691  3.48654535]\n",
      "➡️➡️➡️⬇️⬇️\n",
      "⬆️⏫️⏩️⬇️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n",
      "7.473914205728017\n",
      "qtable[0]: [2.13786691 3.48637956 2.82405631 2.13786691 3.13786691]\n",
      "qtable[1]: [ 2.48654535  3.87375507 -6.86213309  3.13786691  3.48654535]\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏩️➡️⬇️\n",
      "⬆️⬅️⏬➡️⬇️\n",
      "⬆️⏩️✅⏪⬇️\n",
      "⬆️⏩️⬆️⬅️⬅️\n"
     ]
    }
   ],
   "source": [
    "#通过采样的方法计算action value，model free的话意味着不知道整个gridworld的概率了，所以不能直接套贝尔曼方程迭代求解\n",
    "policy = np.eye(5)[np.random.randint(0,5,size=(rows*columns))] \n",
    "gridworld.show()\n",
    "gridworld.showPolicy(policy)\n",
    "print(\"random policy\")\n",
    "\n",
    "\n",
    "trajectorySteps = 100\n",
    "qtable = np.zeros((rows*columns,5))    #生成Qtable，也就是action-value-table\n",
    "qtable_pre = qtable.copy()+1\n",
    "while(np.sum((qtable_pre-qtable)**2)>0.001):\n",
    "    print(np.sum((qtable_pre-qtable)**2))\n",
    "    qtable_pre = qtable.copy()\n",
    "    #通过采样获得action-value的值\n",
    "    for i in range(rows * columns):    #循环每个state\n",
    "        for j in range(5):             #循环每个action\n",
    "            # qtable_rewards = [[0 for j in range(5)] for i in range(rows * columns)] \n",
    "            # qtable_nums =    [[0 for j in range(5)] for i in range(rows * columns)]\n",
    "            Trajectory = gridworld.getTrajectoryScore(nowState=i, action=j, policy=policy, steps=trajectorySteps)\n",
    "            # 注意这里的返回值是大小为(trajectorySteps+1)的元组列表，因为把第一个动作也加入进去了\n",
    "            _, _, score, _, _ = Trajectory[trajectorySteps]\n",
    "            for k in range(trajectorySteps-1,-1,-1):\n",
    "                tmpstate, tmpaction, tmpscore,_ ,_  = Trajectory[k]\n",
    "                score = score*gamma + tmpscore  #细节从后往前优化算法\n",
    "                # qtable_rewards[tmpstate][tmpaction] += score\n",
    "                # qtable_nums[tmpstate][tmpaction] += 1\n",
    "                qtable[tmpstate][tmpaction] = score  #first visit\n",
    "                \n",
    "            #qtable[i,j] = score\n",
    "\n",
    "    print(\"qtable[0]:\", qtable[0])\n",
    "    print(\"qtable[1]:\", qtable[1])\n",
    "    policy = np.eye(5)[np.argmax(qtable,axis=1)]  #qtable的最优值作为更新策略，并用独热码来表示\n",
    "    \n",
    "    gridworld.showPolicy(policy)\n",
    "    # print(np.sum((qtable_pre-qtable)**2))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6845-1a7a-485a-a5b1-788fd9adf37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69941fef-966e-4a9a-9e0c-f9c47e83fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
