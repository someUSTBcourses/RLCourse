# 2024年度《强化学习算法实践》课程大作业

以下的三个大作业（报告形式1个，练习实践形式2个）可以选择其中一项来完成。写的报告可以按照自己的理解，没有特别长度的要求，报告的latex模版在不同的作业下面，也在overleaf上共享了：https://www.overleaf.com/5615922614qhqmtcrjscwm#cbbc08。 本次作业提交没有模版的要求。

提交的位置：

https://yunpan.ustb.edu.cn/link/AAE0CD494F52C941C881BF2FBF50D0B69B

文件夹名：2024年秋季

有效期限：永久有效

提取码：o1DV


## 报告形式: 按照自研方向或者课堂分类的方向，完成一个RL的报告，ppt格式或者word格式均可。

要求：
1. 报告的题目自定，需要说明解决的问题，强化学习的方法，方法的原理，特别是最后自己提出两个问题（与方法相关），并给出回答。（代码不是必须的）

2. 报告的材料可以按照想解决的问题，找到相关的论文，或者自己提出想解决的方案。

3. 提交文档到:云盘指定位置下有个“报告”的目录

## 题目实际练习： 用了其他学校课程中的题目

### 题目1:  Q-Learning算法
#### 环境描述

本次作业的环境为网格世界(gridworld)，玩家可以通过选择动作来移动人物，走到出口。如下图，人形网格是由玩家控制的单位，白色砖状网格为不可通行的墙壁，右下角的褐色网格是出口，其余黑色部分表示可以通行的通道。

<img src="RL_HW1\code\imgs\0.jpeg" alt="0" style="zoom:200%;" width=200 height=200/>


玩家得到的观测：一个二维数组(x,y)，表示玩家所处的位置坐标。

可执行的动作：{0，1，2，3}分别表示上下左右四个方向移动。

奖励：玩家在游戏中每移动一步会得到-1的奖励，走到出口时将额外得到100的奖励。

游戏目标：尽可能达到高的累计奖励/移动小人以尽可能少的步数到达出口。

#### 任务描述

请完成：

1. 与环境交互，在环境中采样并记录轨迹。
2. 依据Q-learning算法，学习一个移动小人走到出口的策略。
4. 绘制你实现的Q-learning算法的性能图（训练所用的样本与得到的累计奖励的关系图）

#### 代码描述

代码文件夹code由'main.py', 'arguments.py', 'algo.py','env.py' 组成。

'main.py'：包含了代码的主要结构，包括环境初始化、如何与环境交互的样例等等。**你需要在其中实现Q-learning算法的相关部分**，并用你的策略(agent.select_action)来玩游戏并进行性能测试。

'arguments.py': 包含了默认的参数，可以修改。

'algo.py': 包含了待填充的Q-learning算法QAgent，**请继承其中的QAgent来实现你自己的算法**。

'env.py'：包含了内置的环境，请勿修改。

运行代码前请安装：

numpy、argparse、pickle、gym、matplotlib、PIL



## 提交方式

完成的作业请提交到云盘指定位置下的“题目1”目录下。上传的格式为一份压缩文件，命名为'学号+姓名'的格式，例如'MG20370001张三.zip'。文件中需包含  'main.py', 'arguments.py', 'algo.py','env.py', 'performance.png' 和'Document.pdf' （一份pdf格式的说明文档），文档内容至少需要包含：

1. 实验效果说明。
2. 如何复现实验效果。
3. Q-learning算法的实现说明。
4. 如果有相关的改进，也请在其中说明。

### 题目2:实现Model-based Q-learning算法

#### 环境描述

本次作业使用环境为网格世界(gridworld)，玩家可以通过选择动作来移动人物，走到出口。和题目1唯一的区别在于输出的状态包括了额外的一维特征，表示agent是否拿到了钥匙。agent需要先拿到钥匙（坐标在（0,7）），然后走到出口才算通关。



#### 实验描述

##### 实验探究1：实现Dyna-Q 算法，并通过调节参数找到算法可提升的性能极限。

伪代码如下：
```latex
Require: initialized Q(s,a) and Model(s,a) for all s \in S and a \in A. A given reward function R and terminal function D.
s = env.reset()
While True:
	while not done:
        a = epsilon-greedy(s,Q)
        s',r, done = env.step(a)
        update Q: Q(s,a) <- \alpha [r + (1 - done) * \gamma max_a' Q(s',a') - Q(s,a)]
        update Model: Model(s,a) <- s'
        s = s'
        if done:
            s = env.reset()
    repeat n times:
        s_m = random previously ovserved state
        a_m = random action previously taken in s_m
        s'_m = Model(s_m,a_m)
        r_m = R(s_m, a_m)
        update Q: Q(s_m,a_m) <- \alpha [r_m + (1 - done) * \gamma max_a' Q(s'_m,a'_m) - Q(s_m,a_m)]
        
```

实验要求：

1. 写完代码后，请从n=0开始（即纯 model-free 方法），尝试调试不同的参数 $n$ 记录算法的收敛时间，和所消耗的样本量。得出一个经验性的 $n^\* $ 的粗略估计，表示若 $n$的取值 $n > n^\*$  算法收敛所消耗的样本量不再有明显的下降。
2. 请在实验报告中展示你所尝试的参数和对应的实验结果。

Note: 

1. 由于环境的转移是确定性的，Model 也可以用table 来进行记录和更新
2. policy 的学习部分，可以使用你在HW2中的实现

#### 实验探究2：用神经网络来预测环境Model， 实现简单的Model-based 算法，完成以下三个探究问题

伪代码如下：

```latex
initialize Q(s,a) and Model(s,a) for all s \in S and a \in A. A given reward function R and terminal function D.
s = env.reset()
for iter in T:
	while not done:
        a = epsilon-greedy(s,Q)
        s',r = env.step(a)
        update Q: Q(s,a) = Q(s,a) + \alpha [r + (1 - done) * \gamma max_a' Q(s',a') - Q(s,a)]
        s = s'
        if done:
            s = env.reset()
    
    repeat m times:
	    Model.train_transition()
    if iter > start_planning:
    	repeat n times:
            s_m = random previously ovserved state
            repeat h times:
                a_m = epsilon-greedy(s_m,Q)
                s'_m = Model(s_m,a_m)
                r_m = R(s_m, a_m, s'_m)
                done = D(s_m, a_m, s'_m)
                update Q: Q(s_m,a_m) = Q(s_m,a_m) + (1 - done) * \alpha [r_m + \gamma max_a' Q(s'_m,a'_m) - Q(s_m,a_m)]
                if done:
                    break
	 
```

##### 实验1：算法调试

1. 该实验的Model 相关接口及其实现已经写好，调节算法的参数，寻找你能找到的达到最好效果的参数组合
   1.  n (采样的轨迹条数)，
   2.  start_planning (开始使用model based 提高样本利用率)，
   3.  h （一条轨迹执行的长度）
   4.  m （转移训练的频率）
   5.  ... 其他你发现的有影响的参数
2. 请在实验报告中展示你所尝试的有显著差异的参数组合和实验结果

##### 实验2： 改进算法

改进1：尝试改进Model的学习流程，强化对稀疏/奖励变化相关的数据的学习，可参考下面的代码：

```python

    def store_transition(self, s, a, r, s_):
        s = self.norm_s(s)
        s_ = self.norm_s(s_)
        self.buffer.append([s, a, r, s_])
        # 新增部分
        if s[-1] - s_[-1] != 0:
            self.sensitive_index.append(len(self.buffer) - 1)

    def train_transition(self, batch_size):
        s_list = []
        a_list = []
        r_list = []
        s_next_list = []
        for _ in range(batch_size):
            idx = np.random.randint(0, len(self.buffer))
            s, a, r, s_ = self.buffer[idx]
            s_list.append(s)
            a_list.append([a])
            r_list.append(r)
            s_next_list.append(s_)
        # 新增部分
        if len(self.sensitive_index) > 0:
            for _ in range(batch_size):
                idx = np.random.randint(0, len(self.sensitive_index))
                idx = self.sensitive_index[idx]
                s, a, r, s_ = self.buffer[idx]
                s_list.append(s)
                a_list.append([a])
                r_list.append(r)
                s_next_list.append(s_)
        x_mse = self.sess.run([self.x_mse,  self.opt_x], feed_dict={
            self.x_ph: s_list, self.a_ph: a_list, self.x_next_ph: s_next_list
        })[:1]
        return x_mse
```

改进2：对策略的学习过程做额外的约束：

```latex
Q(s,a) = Q(s,a) + \alpha [r + (1 - done) * \gamma max_a' Q(s',a') - Q(s,a)]
Q(s,a) = np.clip(Q(s,a), -100, 100)
```

分别尝试两个改进，重新调节该探究问题 中实验1 的参数组合，最优的参数和对应的性能是否发生变化？若有变化，发生了什么变化

（Optional）：可以尝试其他任意的改进，并展示你的改进带来的性能提升



## 提交方式

完成的作业请提交到云盘指定位置下的“题目2”目录下。上传的格式为一份压缩文件，命名为'学号+姓名'的格式，例如'MG21370001张三.zip'。文件中需包含  'main.py', 'arguments.py', 'algo.py','env.py', 'performance.png' 和'Document.pdf' （一份pdf格式的说明文档）