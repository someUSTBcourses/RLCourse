---
marp: true
# theme: uncover
theme: default
# _class: invert
footer: '可解释的深度强化学习，基于可解释优化的深度强化学习'
paginate: true
style: |
  section a {
      font-size: 100px;
      text-align: center;
  }

---
![img](..\images\models.png)

## **可解释强化学习**
#### 目标
* **可解释强化学习综述：概念、算法和挑战**：
* **因果推理与强化学习** 
* **DRL的奖励解释**  ： 多智能体中的奖励分配-优化协作？
* **model-based强化学习** 
* **分层强化学习**
---

## 可解释强化学习综述

* 中文的参考资料: ../强化学习可解释性基础问题探索和方法综述-2020.pdf (github上已经下载)

1. 可解释强化学习的综述论文：
A Survey on Explainable Reinforcement Learning: Concepts,Algorithms, and Challenges (2022) - 浙大
2. 可解释强化学习的框架
* 论文：Evaluating Explanation Without Ground Truth in Interpretable Machine Learning  (2019)
  
---
## 可解释机器学习的方法是否能用于RL/DRL  - grad-cam
* 中文的参考资料：深度学习模型可解释性研究综述:2篇
* 3. 总结下中文综述  + Grad-CAM与机器学习： 可视化技术
  * Learning Deep Features for Discriminative Localization  论文地址：https:// arxiv.org/pdf/1512.0415
  * 讲解的笔记；https:// zhuanlan.zhihu.com/p/479485138
  
---
## 可解释机器学习的方法是否能用于RL/DRL  - grad-cam

* 4.总结 grad-cam是否能用于DRL
  * 笔记；https:// zhuanlan.zhihu.com/p/493769313
  * 论文  Rosynski, M., Kirchner, F., & Valdenegro-Toro, M. (2020). Are Gradient-based Saliency Maps Useful in Deep Reinforcement Learning? ”I Can't Believe It's Not Better!” NeurIPS 2020 Workshop
    * 相关论文：H. -T. Joo and K. -J. Kim, "Visualization of Deep Reinforcement Learning using Grad-CAM: How AI Plays Atari Games?,"2019 IEEE Conference on Games (CoG)  2页
    * 相关实现  Github: Mateus224 / Visual-Explanation-in-Deep-Reinforcement-Learning (2020) 

---

## 可解释机器学习的方法是否能用于RL/DRL - shapley
参考的项目：
* shapley值的相关项目：https:// github.com/slundberg/shap
* 相关代码介绍的一个博客；https:// e0hyl.github.io/BLOG-OF-E0/posts/   https:// e0hyl.github.io/BLOG-OF-E0/LIMEandSHAP/
* 课程：https:// christophm.github.io/interpretable-ml-book/shapley.html
---
## 可解释机器学习的方法是否能用于RL/DRL - shapley
* 5.事后解释方法-LIME
   * “Why Should I Trust You?” Explaining the Predictions of Any Classififier
* 6.事后解释方法-shapley值的论文
   * The Shapley Value in Machine Learning  
   * 论文地址；https:// arxiv.org/abs/2202.05594
* 7.From Shapley back to Pearson: Hypothesis Testing via the Shapley Value   - https:// paperswithcode.com/paper/from-shapley-back-to-pearson-hypothesis
   
---
## 因果推理与强化学习
参考资料： 可以不看
* 顶会中的因果推理与强化学习：https:// zhuanlan.zhihu.com/p/588978133
* 因果推断常用工具库：https:// zhuanlan.zhihu.com/p/600370278
* 一个中文论文：基于因果建模的强化学习控制

----
## 因果推理与强化学习
* 8.因果强化学习综述：Causal Reinforcement Learning: A Survey - 清华大学
  * 论文地址：https:// openreview.net/forum?id=iATMbh8mhD&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR
* 9.迁移RL中的因果推理：AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning
  * 论文地址：https:// arxiv.org/abs/2107.02729
* 10.Factored Adaptation for Non-Stationary Reinforcement Learning
* 11.AAAI 2023-用因果推理做部分可观测强化学习 主要还是因果推理 Fast Counterfactual Inference for History-Based Reinforcement Learning
  * 笔记：https:// zhuanlan.zhihu.com/p/586158744
* 12.Causality-driven Hierarchical Structure Discovery for Reinforcement Learning
   
---
## 基于shapley实现DRL中的奖励解释
* 13.Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values  (https:// paperswithcode.com/paper/collective-explainable-ai-explaining)
* 14.Shapley Q-value: A Local Reward Approach to Solve Global Reward Games (https:// arxiv.org/pdf/1907.05707v6.pdf)
* 15.SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning (https:// paperswithcode.com/paper/shaq-incorporating-shapley-value-theory-into) - 陈丽萍
* 16.Towards a more efficient computation of individual attribute and policy contribution for post-hoc explanation of cooperative multi-agent systems using Myerson values(https:// arxiv.org/pdf/2212.03041v1.pdf)

---
## Model-based RL
* 17.model-based RL综述：A Survey on Model-based Reinforcement Learning (https:// arxiv.org/abs/2206.09328)
* 18.代码实践
  * 动手学强化学习第16章: 模型预测控制 (https:// hrl.boyuai.com/chapter/3/)
  * 动手学强化学习第17章:基于模型的策略优化(https: //hrl.boyuai.com/chapter/3/)
---
